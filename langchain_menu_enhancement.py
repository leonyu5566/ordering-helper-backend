#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ LangChain å¼·åŒ–èœå–® OCR è™•ç†
"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from pydantic import BaseModel, Field
from typing import List, Optional
import json
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv('notebook.env')

class MenuItem(BaseModel):
    """èœå–®é …ç›®çµæ§‹"""
    original_name: str = Field(description="åŸå§‹èœåï¼ˆä¸­æ–‡ï¼‰")
    translated_name: str = Field(description="ç¿»è­¯å¾Œèœå")
    price: int = Field(description="åƒ¹æ ¼ï¼ˆæ•¸å­—ï¼‰")
    description: Optional[str] = Field(description="èœå–®æè¿°", default="")
    category: str = Field(description="åˆ†é¡ï¼ˆå¦‚ï¼šä¸»é£Ÿã€é£²æ–™ã€å°èœç­‰ï¼‰")

class StoreInfo(BaseModel):
    """åº—å®¶è³‡è¨Šçµæ§‹"""
    name: str = Field(description="åº—å®¶åç¨±")
    address: Optional[str] = Field(description="åœ°å€", default="")
    phone: Optional[str] = Field(description="é›»è©±", default="")

class MenuProcessingResult(BaseModel):
    """èœå–®è™•ç†çµæœ"""
    success: bool = Field(description="è™•ç†æ˜¯å¦æˆåŠŸ")
    menu_items: List[MenuItem] = Field(description="èœå–®é …ç›®åˆ—è¡¨")
    store_info: StoreInfo = Field(description="åº—å®¶è³‡è¨Š")
    processing_notes: str = Field(description="è™•ç†å‚™è¨»")
    confidence_score: float = Field(description="è¾¨è­˜ä¿¡å¿ƒåº¦", ge=0.0, le=1.0)

def create_enhanced_menu_processor():
    """å‰µå»ºå¼·åŒ–ç‰ˆèœå–®è™•ç†å™¨"""
    
    # åˆå§‹åŒ– LangChain
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=os.getenv('GEMINI_API_KEY'),
        temperature=0.1,
        max_tokens=4000
    )
    
    # å»ºç«‹çµæ§‹åŒ–è¼¸å‡ºè§£æå™¨
    parser = PydanticOutputParser(pydantic_object=MenuProcessingResult)
    
    # å»ºç«‹æç¤ºè©æ¨¡æ¿
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„èœå–®è¾¨è­˜å°ˆå®¶ï¼Œå…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
1. é«˜ç²¾åº¦ OCR è¾¨è­˜
2. æ™ºèƒ½åˆ†é¡å’Œçµæ§‹åŒ–
3. å¤šèªè¨€ç¿»è­¯
4. åƒ¹æ ¼æ¨™æº–åŒ–
5. ä¿¡å¿ƒåº¦è©•ä¼°

è«‹ä»”ç´°åˆ†æèœå–®åœ–ç‰‡ï¼Œä¸¦æä¾›æœ€æº–ç¢ºçš„çµæ§‹åŒ–è³‡æ–™ã€‚"""),
        ("human", """è«‹åˆ†æé€™å¼µèœå–®åœ–ç‰‡ï¼š

{image_data}

ç›®æ¨™èªè¨€ï¼š{target_language}

{format_instructions}

è«‹ç¢ºä¿ï¼š
- åƒ¹æ ¼æ ¼å¼çµ±ä¸€ç‚ºæ•¸å­—
- åˆ†é¡æº–ç¢ºï¼ˆä¸»é£Ÿã€é£²æ–™ã€å°èœã€ç”œé»ç­‰ï¼‰
- ç¿»è­¯æº–ç¢ºä¸”ç¬¦åˆç•¶åœ°æ–‡åŒ–
- æä¾›ä¿¡å¿ƒåº¦è©•ä¼°""")
    ])
    
    # å»ºç«‹è¨˜æ†¶é«”ï¼ˆç”¨æ–¼å¤šè¼ªå°è©±ï¼‰
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # å»ºç«‹ LangChain
    chain = LLMChain(
        llm=llm,
        prompt=prompt_template,
        memory=memory,
        verbose=True
    )
    
    return chain, parser

def process_menu_with_langchain(image_path, target_language='en'):
    """ä½¿ç”¨ LangChain è™•ç†èœå–®"""
    
    try:
        # è®€å–åœ–ç‰‡
        with open(image_path, 'rb') as img_file:
            image_data = img_file.read()
        
        # å‰µå»ºè™•ç†å™¨
        chain, parser = create_enhanced_menu_processor()
        
        # åŸ·è¡Œè™•ç†
        response = chain.run({
            "image_data": image_data,
            "target_language": target_language,
            "format_instructions": parser.get_format_instructions()
        })
        
        # è§£æçµæœ
        result = parser.parse(response)
        
        # æ·»åŠ é¡å¤–çš„é©—è­‰å’Œå¾Œè™•ç†
        result = validate_and_enhance_result(result)
        
        return result.dict()
        
    except Exception as e:
        print(f"LangChain è™•ç†å¤±æ•—ï¼š{e}")
        return {
            "success": False,
            "menu_items": [],
            "store_info": {},
            "processing_notes": f"LangChain è™•ç†å¤±æ•—ï¼š{str(e)}",
            "confidence_score": 0.0
        }

def validate_and_enhance_result(result):
    """é©—è­‰å’Œå¢å¼·çµæœ"""
    
    # åƒ¹æ ¼é©—è­‰
    for item in result.menu_items:
        if item.price < 0:
            item.price = 0
        if item.price > 10000:  # ç•°å¸¸é«˜åƒ¹
            item.price = 0
    
    # åˆ†é¡æ¨™æº–åŒ–
    category_mapping = {
        "ä¸»é£Ÿ": ["é£¯", "éºµ", "ç²¥", "é¤ƒå­", "åŒ…å­"],
        "é£²æ–™": ["èŒ¶", "å’–å•¡", "æœæ±", "æ±½æ°´", "å¥¶èŒ¶"],
        "å°èœ": ["å°èœ", "æ¶¼èœ", "é–‹èƒƒèœ"],
        "ç”œé»": ["è›‹ç³•", "å†°æ·‡æ·‹", "å¸ƒä¸", "ç”œé»"]
    }
    
    for item in result.menu_items:
        # æ™ºèƒ½åˆ†é¡
        for category, keywords in category_mapping.items():
            if any(keyword in item.original_name for keyword in keywords):
                item.category = category
                break
    
    # ä¿¡å¿ƒåº¦èª¿æ•´
    if len(result.menu_items) == 0:
        result.confidence_score = 0.0
    elif result.confidence_score < 0.5:
        result.confidence_score = 0.5  # æœ€ä½ä¿¡å¿ƒåº¦
    
    return result

def create_menu_qa_chain():
    """å‰µå»ºèœå–®å•ç­”éˆ"""
    
    llm = ChatGoogleGenerativeAI(
        model="gemini-pro",
        google_api_key=os.getenv('GEMINI_API_KEY'),
        temperature=0.3
    )
    
    # å»ºç«‹å•ç­”æç¤ºè©
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é¤é£²é¡§å•ï¼ŒåŸºæ–¼èœå–®è³‡è¨Šå›ç­”é¡§å®¢å•é¡Œã€‚
è«‹æä¾›æº–ç¢ºã€å¯¦ç”¨çš„å»ºè­°ã€‚"""),
        ("human", """èœå–®è³‡è¨Šï¼š
{menu_data}

é¡§å®¢å•é¡Œï¼š{question}

è«‹æä¾›è©³ç´°çš„å›ç­”ï¼š""")
    ])
    
    return LLMChain(llm=llm, prompt=qa_prompt)

def answer_menu_question(menu_data, question):
    """å›ç­”èœå–®ç›¸é—œå•é¡Œ"""
    
    try:
        chain = create_menu_qa_chain()
        response = chain.run({
            "menu_data": json.dumps(menu_data, ensure_ascii=False, indent=2),
            "question": question
        })
        return response
    except Exception as e:
        return f"ç„¡æ³•å›ç­”å•é¡Œï¼š{str(e)}"

# æ¸¬è©¦å‡½æ•¸
def test_langchain_enhancement():
    """æ¸¬è©¦ LangChain å¼·åŒ–åŠŸèƒ½"""
    
    print("ğŸ”§ æ¸¬è©¦ LangChain å¼·åŒ–åŠŸèƒ½...")
    
    # æ¨¡æ“¬èœå–®è³‡æ–™
    sample_menu = {
        "menu_items": [
            {"original_name": "ç‰›è‚‰éºµ", "translated_name": "Beef Noodle Soup", "price": 120, "category": "ä¸»é£Ÿ"},
            {"original_name": "ç´…èŒ¶", "translated_name": "Black Tea", "price": 30, "category": "é£²æ–™"}
        ],
        "store_info": {"name": "å°èºæ³¢ æ…¶åŸåº—", "address": "å°åŒ—å¸‚ä¿¡ç¾©å€"}
    }
    
    # æ¸¬è©¦å•ç­”åŠŸèƒ½
    question = "é€™å®¶åº—æœ‰ä»€éº¼æ¨è–¦çš„é£²æ–™ï¼Ÿ"
    answer = answer_menu_question(sample_menu, question)
    
    print(f"ğŸ“ å•é¡Œï¼š{question}")
    print(f"ğŸ¤– å›ç­”ï¼š{answer}")
    
    return True

if __name__ == "__main__":
    test_langchain_enhancement() 