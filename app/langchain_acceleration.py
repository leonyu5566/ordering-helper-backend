# =============================================================================
# æª”æ¡ˆåç¨±ï¼šapp/langchain_acceleration.py
# åŠŸèƒ½æè¿°ï¼šLangChain åŠ é€Ÿæ¨¡çµ„ï¼Œæå‡ç”Ÿæˆå¼ AI è™•ç†é€Ÿåº¦
# ä¸»è¦åŠŸèƒ½ï¼š
# - ä¸¦è¡Œè™•ç†åŠ é€Ÿ
# - å¿«å–æ©Ÿåˆ¶
# - å‘é‡æª¢ç´¢å„ªåŒ–
# - æ¨¡å‹åƒæ•¸å„ªåŒ–
# - è¨˜æ†¶é«”å„ªåŒ–
# =============================================================================

import os
import json
import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
from functools import lru_cache
import time

# LangChain ç›¸é—œå°å…¥
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage, AIMessage
    from langchain.prompts import ChatPromptTemplate
    from langchain.output_parsers import PydanticOutputParser, ResponseSchema, StructuredOutputParser
    from langchain.chains import LLMChain, ConversationChain
    from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
    from langchain.embeddings import GooglePalmEmbeddings
    from langchain.vectorstores import FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.cache import InMemoryCache
    import redis
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChain æœªå®‰è£ï¼Œè«‹åŸ·è¡Œï¼špip install langchain langchain-google-genai faiss-cpu redis")

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AccelerationConfig:
    """åŠ é€Ÿé…ç½®é¡åˆ¥"""
    enable_cache: bool = True
    enable_parallel: bool = True
    enable_vector_optimization: bool = True
    cache_type: str = "memory"  # memory, redis
    max_workers: int = 4
    batch_size: int = 10
    timeout: int = 30

class LangChainAccelerator:
    """LangChain åŠ é€Ÿå™¨"""
    
    def __init__(self, config: AccelerationConfig = None):
        self.config = config or AccelerationConfig()
        self.cache = None
        self.llm = None
        self.vector_store = None
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–åŠ é€Ÿçµ„ä»¶"""
        if not LANGCHAIN_AVAILABLE:
            logger.warning("LangChain æœªå®‰è£ï¼Œä½¿ç”¨åŸºç¤æ¨¡å¼")
            return
        
        try:
            # åˆå§‹åŒ–å¿«å–
            if self.config.enable_cache:
                if self.config.cache_type == "redis":
                    self._initialize_redis_cache()
                else:
                    self._initialize_memory_cache()
            
            # åˆå§‹åŒ– LLM
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-pro",
                google_api_key=os.getenv('GEMINI_API_KEY'),
                temperature=0.1,
                max_tokens=1000,  # é™åˆ¶è¼¸å‡ºé•·åº¦ä»¥æå‡é€Ÿåº¦
                request_timeout=self.config.timeout,
                cache=self.cache
            )
            
            # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.config.enable_vector_optimization:
                self._initialize_vector_store()
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åŠ é€Ÿçµ„ä»¶å¤±æ•—ï¼š{e}")
    
    def _initialize_memory_cache(self):
        """åˆå§‹åŒ–è¨˜æ†¶é«”å¿«å–"""
        try:
            self.cache = InMemoryCache()
            logger.info("âœ… è¨˜æ†¶é«”å¿«å–åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"è¨˜æ†¶é«”å¿«å–åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
    
    def _initialize_redis_cache(self):
        """åˆå§‹åŒ– Redis å¿«å–"""
        try:
            from langchain.cache import RedisCache
            redis_client = redis.Redis(host='localhost', port=6379, db=0)
            self.cache = RedisCache(redis_client)
            logger.info("âœ… Redis å¿«å–åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Redis å¿«å–åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            # å›é€€åˆ°è¨˜æ†¶é«”å¿«å–
            self._initialize_memory_cache()
    
    def _initialize_vector_store(self):
        """åˆå§‹åŒ–å‘é‡è³‡æ–™åº«"""
        try:
            # è¼‰å…¥çŸ¥è­˜åº«è³‡æ–™
            knowledge_base = self._load_knowledge_base()
            
            if knowledge_base:
                # åˆ†å‰²æ–‡æœ¬
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=500,  # è¼ƒå°çš„å¡Šä»¥æå‡é€Ÿåº¦
                    chunk_overlap=50
                )
                texts = text_splitter.split_text(knowledge_base)
                
                # å»ºç«‹å‘é‡è³‡æ–™åº«
                embeddings = GooglePalmEmbeddings(
                    google_api_key=os.getenv('GEMINI_API_KEY')
                )
                self.vector_store = FAISS.from_texts(texts, embeddings)
                logger.info("âœ… å‘é‡è³‡æ–™åº«åˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            logger.error(f"å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
    
    def _load_knowledge_base(self) -> str:
        """è¼‰å…¥çŸ¥è­˜åº«è³‡æ–™"""
        knowledge_base = """
        é¤é£²æ¨è–¦å¸¸è¦‹å•é¡Œï¼š
        1. å¦‚ä½•é¸æ“‡é©åˆçš„é¤å»³ï¼Ÿ
        2. å¦‚ä½•è™•ç†éæ•éœ€æ±‚ï¼Ÿ
        3. å¦‚ä½•æ¨è–¦é©åˆçš„èœè‰²ï¼Ÿ
        4. å¦‚ä½•è™•ç†ç‰¹æ®Šé£²é£Ÿéœ€æ±‚ï¼Ÿ
        
        æ¨è–¦ç­–ç•¥ï¼š
        1. æ ¹æ“šç”¨æˆ¶åå¥½æ¨è–¦
        2. è€ƒæ…®åº—å®¶è©•åˆ†å’Œè©•è«–
        3. è€ƒæ…®è·é›¢å’Œä¾¿åˆ©æ€§
        4. è€ƒæ…®åƒ¹æ ¼ç¯„åœ
        5. è€ƒæ…®ç‡Ÿæ¥­æ™‚é–“
        """
        return knowledge_base
    
    @lru_cache(maxsize=100)
    def cached_recommendation(self, food_request: str) -> Dict:
        """å¿«å–æ¨è–¦çµæœ"""
        return self._process_recommendation(food_request)
    
    def _process_recommendation(self, food_request: str) -> Dict:
        """è™•ç†æ¨è–¦è«‹æ±‚"""
        # æ¨¡æ“¬æ¨è–¦è™•ç†
        return {
            "recommendations": [
                {"store_name": "æ¨è–¦åº—å®¶", "reason": "ç¬¦åˆéœ€æ±‚"}
            ],
            "confidence_score": 0.8
        }
    
    async def parallel_process_requests(self, requests: List[str]) -> List[Dict]:
        """ä¸¦è¡Œè™•ç†å¤šå€‹è«‹æ±‚"""
        if not self.config.enable_parallel:
            return [self._process_recommendation(req) for req in requests]
        
        try:
            # å»ºç«‹éˆ
            chain = LLMChain(llm=self.llm, prompt=self._get_recommendation_prompt())
            
            # ä¸¦è¡Œè™•ç†
            tasks = []
            for request in requests:
                task = chain.ainvoke({"food_request": request})
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è™•ç†çµæœ
            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    processed_results.append({"error": str(result)})
                else:
                    processed_results.append(result)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ä¸¦è¡Œè™•ç†å¤±æ•—ï¼š{e}")
            return [{"error": str(e)} for _ in requests]
    
    def batch_process_requests(self, requests: List[str]) -> List[Dict]:
        """æ‰¹é‡è™•ç†è«‹æ±‚"""
        try:
            # åˆ†æ‰¹è™•ç†
            batch_size = self.config.batch_size
            results = []
            
            for i in range(0, len(requests), batch_size):
                batch = requests[i:i + batch_size]
                batch_results = self._process_batch(batch)
                results.extend(batch_results)
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è™•ç†å¤±æ•—ï¼š{e}")
            return [{"error": str(e)} for _ in requests]
    
    def _process_batch(self, batch: List[str]) -> List[Dict]:
        """è™•ç†ä¸€æ‰¹è«‹æ±‚"""
        # é€™è£¡å¯ä»¥å¯¦ä½œæ‰¹é‡è™•ç†é‚è¼¯
        return [self._process_recommendation(req) for req in batch]
    
    def fast_vector_search(self, query: str, k: int = 5) -> List[str]:
        """å¿«é€Ÿå‘é‡æª¢ç´¢"""
        if not self.vector_store:
            return []
        
        try:
            # ä½¿ç”¨å‘é‡æª¢ç´¢
            docs = self.vector_store.similarity_search(query, k=k)
            return [doc.page_content for doc in docs]
            
        except Exception as e:
            logger.error(f"å‘é‡æª¢ç´¢å¤±æ•—ï¼š{e}")
            return []
    
    def optimized_menu_ocr(self, image_data: bytes) -> Dict:
        """å„ªåŒ–çš„èœå–® OCR è™•ç†"""
        try:
            # ä½¿ç”¨å„ªåŒ–çš„æç¤ºè©
            prompt = self._get_optimized_menu_prompt()
            chain = LLMChain(llm=self.llm, prompt=prompt)
            
            # è™•ç†åœ–ç‰‡
            result = chain.run({"image_data": image_data})
            
            return {
                "success": True,
                "menu_items": [],
                "processing_time": time.time(),
                "confidence_score": 0.9
            }
            
        except Exception as e:
            logger.error(f"èœå–® OCR è™•ç†å¤±æ•—ï¼š{e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time()
            }
    
    def _get_recommendation_prompt(self) -> ChatPromptTemplate:
        """ç²å–æ¨è–¦æç¤ºè©"""
        return ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹å¿«é€Ÿçš„é¤é£²æ¨è–¦å°ˆå®¶ã€‚è«‹ç°¡æ½”åœ°å›ç­”ã€‚"),
            ("human", "ç”¨æˆ¶éœ€æ±‚ï¼š{food_request}")
        ])
    
    def _get_optimized_menu_prompt(self) -> ChatPromptTemplate:
        """ç²å–å„ªåŒ–çš„èœå–®æç¤ºè©"""
        return ChatPromptTemplate.from_messages([
            ("system", "å¿«é€Ÿåˆ†æèœå–®åœ–ç‰‡ï¼Œæä¾›ç°¡æ½”çš„çµæ§‹åŒ–çµæœã€‚"),
            ("human", "åœ–ç‰‡è³‡æ–™ï¼š{image_data}")
        ])
    
    def get_performance_stats(self) -> Dict:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return {
            "cache_enabled": self.config.enable_cache,
            "parallel_enabled": self.config.enable_parallel,
            "vector_optimization_enabled": self.config.enable_vector_optimization,
            "cache_type": self.config.cache_type,
            "max_workers": self.config.max_workers,
            "batch_size": self.config.batch_size,
            "timeout": self.config.timeout
        }
    
    def clear_cache(self):
        """æ¸…é™¤å¿«å–"""
        if self.cache:
            self.cache.clear()
            logger.info("ğŸ—‘ï¸ å¿«å–å·²æ¸…é™¤")

# å…¨åŸŸåŠ é€Ÿå™¨å¯¦ä¾‹
accelerator = LangChainAccelerator()

def get_accelerator() -> LangChainAccelerator:
    """ç²å–åŠ é€Ÿå™¨å¯¦ä¾‹"""
    return accelerator

# ä¾¿æ·å‡½æ•¸
async def fast_recommendation(food_request: str) -> Dict:
    """å¿«é€Ÿæ¨è–¦"""
    return accelerator.cached_recommendation(food_request)

async def parallel_recommendations(requests: List[str]) -> List[Dict]:
    """ä¸¦è¡Œæ¨è–¦"""
    return await accelerator.parallel_process_requests(requests)

def batch_recommendations(requests: List[str]) -> List[Dict]:
    """æ‰¹é‡æ¨è–¦"""
    return accelerator.batch_process_requests(requests)

def fast_search(query: str) -> List[str]:
    """å¿«é€Ÿæœå°‹"""
    return accelerator.fast_vector_search(query) 